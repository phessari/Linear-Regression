{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this code, we imply Linear Regression. To do so, we use Boston Housing Dataset which can be downloaded from sklearn datasets. \n",
    "\n",
    "The Boston Housing Dataset consists of price of houses in various places in Boston. Here is the list of all attribute:\n",
    "\n",
    "    CRIM      per capita crime rate by town\n",
    "    ZN        proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "    INDUS     proportion of non-retail business acres per town\n",
    "    CHAS      Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "    NOX       nitric oxides concentration (parts per 10 million)\n",
    "    RM        average number of rooms per dwelling\n",
    "    AGE       proportion of owner-occupied units built prior to 1940\n",
    "    DIS       weighted distances to five Boston employment centres\n",
    "    RAD       index of accessibility to radial highways\n",
    "    TAX       full-value property-tax rate per 10,000 dollar\n",
    "    PTRATIO   pupil-teacher ratio by town\n",
    "    B         1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "    LSTAT     lower status of the population\n",
    "    MEDV      Median value of owner-occupied homes in $1000's\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use linear regression to find the linear relationship between the features and target variable. Since \n",
    "we use only two features of data, the relation would be \n",
    "\n",
    "y = a_0 + a_1 x_1 + a_2 x_2 \n",
    "\n",
    "In this code we will find the coefficients a_0, a_1 and a_2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 2)\n",
      "The model is: y = 22.485628113468223 + -0.35207831564026765 x_1 + 0.11610909184400937 x_2\n"
     ]
    }
   ],
   "source": [
    "# Line Fitting\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston = load_boston()  \n",
    "X = boston.data[:, 0:2]   \n",
    "y = boston.target           \n",
    "\n",
    "#print(boston.feature_names)       # features name\n",
    "#print(boston.DESCR)               # statistical description of the data\n",
    "regression = LinearRegression()\n",
    "\n",
    "model = regression.fit(X, y)\n",
    "\n",
    "print(\"The model is: y =\", model.intercept_,'+', model.coef_[0],\"x_1 +\", model.coef_[1], \"x_2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible that sometimes the target variable not only depends on the feature variables x_1 and x_2, but also depends on the interaction between x_a and x_2, i.e., x_1 * x_2. In this case we use PolynomialFeatures command with interaction_only=True to include a new variable x_3 = x_1*x_2. The model becomes \n",
    "y = a_0 + a_1 x_1 + a_2 x_2 + a_3 x_3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is: 22.07715825584366 + -0.3371515939259498 x_1 +  0.08155746534799126 x_2 + 0.806620004402748 x_3\n"
     ]
    }
   ],
   "source": [
    "# Feature Interaction\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "boston = load_boston()\n",
    "X = boston.data[:, 0:2]\n",
    "y = boston.target\n",
    "\n",
    "interaction = PolynomialFeatures(degree=3, include_bias=False, interaction_only=True)\n",
    "X_interaction = interaction.fit_transform(X)\n",
    "\n",
    "regression = LinearRegression()\n",
    "\n",
    "model = regression.fit(X_interaction, target)\n",
    "\n",
    "print(\"The model is:\", model.intercept_,'+', model.coef_[0],\"x_1 + \", \n",
    "                          model.coef_[1], \"x_2\", '+', model.coef_[2], \"x_3\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Linear Regression\n",
    "If the relation between target variable and feature is NOT linear, we should apply nonlinear regression. In this case, we are looking for the model of the form \n",
    "\n",
    "y = a_0 + a_1 x + a_2 x^2 + ... + a_d x^d \n",
    "\n",
    "where d is the degree of the polynomial. We use PolynomialFeatures to construct features x^2, ..., x^2 with degree=d. \n",
    "The rest is the same as linear regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is: 25.190479369326752 + -1.1364007230671813 x +  0.023784825366425656 x^2 + -0.00014887208958576755 x^3\n"
     ]
    }
   ],
   "source": [
    "# POLYNOMIAL REGRESSION\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "boston = load_boston()\n",
    "X = boston.data[:, 0:1]\n",
    "y = boston.target\n",
    "\n",
    "poly = PolynomialFeatures(degree=3, include_bias=False )\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "regression = LinearRegression()\n",
    "\n",
    "model = regression.fit(X_poly, target)\n",
    "\n",
    "print(\"The model is:\", model.intercept_,'+', model.coef_[0],\"x + \", \n",
    "                          model.coef_[1], \"x^2\", '+', model.coef_[2], \"x^3\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression\n",
    "In order to reduce variance of linear regression, two regularizations can be used: Ridge and Lasso. In Ridge regularization we add \n",
    "\n",
    "alpha \\sigma_{j=1}^{n} a_j^2 \n",
    "\n",
    "to the cost function, while in Lasso regression we use \n",
    "\n",
    "alpha \\sigma_{j=1}^{n} |a_j|.\n",
    "\n",
    "Here alpha is a hyperparameter. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.92396151,  1.07393055,  0.12895159,  0.68346136, -2.0427575 ,\n",
       "        2.67854971,  0.01627328, -3.09063352,  2.62636926, -2.04312573,\n",
       "       -2.05646414,  0.8490591 , -3.73711409])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RIDGE REGRESSION\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "boston = load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_standard = scaler.fit_transform(feature)\n",
    "\n",
    "regression = Ridge(alpha=0.5)\n",
    "\n",
    "model = regression.fit(X_standard, target)\n",
    "\n",
    "model.coef_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.91987132,  1.06646104,  0.11738487,  0.68512693, -2.02901013,\n",
       "        2.68275376,  0.01315848, -3.07733968,  2.59153764, -2.0105579 ,\n",
       "       -2.05238455,  0.84884839, -3.73066646])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RIDGE REGRESSION WITH CV\n",
    "\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "boston = load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_standard = scaler.fit_transform(feature)\n",
    "\n",
    "regression_cv = RidgeCV(alphas=[0.1, 1.0, 10.0])\n",
    "\n",
    "model = regression_cv.fit(X_standard, target)\n",
    "\n",
    "model.coef_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.11526463,  0.        , -0.        ,  0.39707879, -0.        ,\n",
       "        2.97425861, -0.        , -0.17056942, -0.        , -0.        ,\n",
       "       -1.59844856,  0.54313871, -3.66614361])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LASSO REGRESSION\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "boston = load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_standard = scaler.fit_transform(feature)\n",
    "\n",
    "regression = Lasso(alpha=0.5)\n",
    "\n",
    "model = regression.fit(X_standard, target)\n",
    "\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
